---
title: "Exercise One"
author: "Ali Prasla, Shreya Trivedi, Reece Wooten, Deeksha Yellam"
date: "August 10, 2017"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Bayesian Problems:

###Part A:

$RC = Random Choice Event$

$TC = True Choice Event$

$Y = "Yes" Choice Event$

$N = "No" Choice Event$

####Solve for $P(Y/TC)$

$P(Y/TC) = P(Y n TC) / P(TC)$

$P(TC) = .7$

$P(Y n TC) = P(Y) - P(Y n RC)$

$P(Y) = .65$

$P(Y n RC) = P(RC) * P(Y/RC)$

$P(RC) = .3$

$P(Y/RC) = .5$

P(Y n RC) = 
```{r}
YAndRC = .3 * .5
YAndRC
```


P(Y n TC) = 

```{r}
YAndTC = .65 - YAndRC
YAndTC
```

Finally,

P(Y/TC) =  
```{r}
YAndTC/.7
```

### Part B.
$P = Positive Test Event$
$D = Has Disease Event$

####Solve for $P(D/P)$

$P(D/P) = (P(P/D) * P(D))/P(P)$
$P(P/D) = .993$
$P(D) = .000025$
$P(P) = P(P n D) + P(P n D^c)$

$P(P n D) = P(P/D) * P(D)$
```{r}
PAndD = .993 * .000025
```

$P(P n D^c) = P(P/D^c) * P(D^c)$
$P(P/D^c) = (1 - .9999)$
```{r}
PAndNotD = (1-.9999) * (1-.000025)
```

```{r}
P = PAndD + PAndNotD
```

$P(D / P) = $
```{r}
print((.993 * .000025) / P)
```

Yes. We foresee problems with universial testing. This test effectively has a 20% false positive rate. Yet, to determine whether or not to implement this policy, it is important to look at other factors, like the deadliness of the disease and the efficacy of early treatment. 



## Portfolio Bootstrapping

In this question we have explored below five asset classes:

- US domestic equities (SPY: the S&P 500 stock index)
- US Treasury bonds (TLT)
- Investment-grade corporate bonds (LQD)
- Emerging-market equities (EEM)
- Real estate (VNQ)

We have taken our data from 2007-01-01. After exploring the data we created three portfolios for these assets assuming we have notional $100,000 to invest in one of these portfolios.

1)
First, we explored the properties of five assets we are considering.

Installing relevant libraries. _Quantmod_ helps us to download several years of daily data on these ETFs.
```{r include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)
```


Importing the stocks we are considering
```{r include = FALSE}
mystocks = c("SPY", "TLT", "LQD","EEM","VNQ")
myprices = getSymbols(mystocks, from = "2007-01-01")
```


Adjusting for splits and dividends
```{r}
SPYa = adjustOHLC(SPY)
TLTa = adjustOHLC(TLT)
LQDa = adjustOHLC(LQD)
EEMa=adjustOHLC(EEM)
VNQa=adjustOHLC(VNQ)
```


Looking at close-to-close changes
```{r}
par(bg = "gray")
plot(ClCl(TLTa),main = "US Treasury Daily Returns",ylab = "Return",col = "navy")
plot(ClCl(LQDa),main = "Investment Grade Corporate Bonds Daily Returns",ylab = "Return",col = "navy")
plot(ClCl(SPYa),main = "S&P 500 Daily Returns",ylab = "Return",col = "navy")
plot(ClCl(EEMa),main = "Emerging Market ETF Daily Returns",ylab = "Return",col = "navy")
plot(ClCl(VNQa),main = "REIT Daily Returns",ylab = "Return",col = "navy")
``` 

Combining close to close changes in a single matrix
```{r}
all_returns = cbind(ClCl(SPYa),ClCl(TLTa),ClCl(LQDa),ClCl(EEMa),ClCl(VNQa))
all_returns = as.matrix(na.omit(all_returns))
```

We calculated the sharpe ratio of individual stocks.

```{r}
summary_stocks = cbind(t(t(colMeans(all_returns))),t(t(apply(all_returns,2,sd))))
summary_stocks = cbind(summary_stocks,summary_stocks[,1]/summary_stocks[,2])
colnames(summary_stocks) = c("Average Returns","ST.Dev of Returns","Sharpe Ratio")
summary_stocks
```

These returns can be viewed as draws from the joint distribution
```{r}
pairs(all_returns)
```


The correlation matrix between stocks
```{r}
cor(all_returns)
```

2)
Now we are going to create our first portfolio with even split. We will assign the weight of each asset as 20%. 

```{r}
initial_wealth = 100000
set.rseed(1)    #Setting seed
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    #rebalance portfolio
    holdings = weights * total_wealth
  }
  wealthtracker
}

head(sim1)

```

Profit/loss of our portfolio:

```{r}
par(bg = "gray")
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30,main = "Histogram of Portfolio Returns",xlab = "End of Period Return",ylab = "Frequency",col = "blue")
abline(v = mean(sim1[,n_days]) - initial_wealth,col = "red",lw = 3)
```

Calculating 5% value at risk for this portfolio

```{r}
quantile(sim1[,n_days], 0.05) - initial_wealth
```

Hence for this portfolio the average return is $100946.6 and value at risk is $6297.993  


3) Now we are going to create a safer portfolio which reduces our risk. To create this portfolio, we first calculated the average returns and risk involved with each asset had we invested all our wealth into that portfolio. Based on the results we tried different combinations and came up with weights which increased the return of portfolio and minimized the risk involved. Below are the weights:
TLT - 20%
LQD - 30%
SPY - 50%
EEM -  0%
VMQ -  0%

```{r}
initial_wealth = 100000
set.rseed(1)  #Setting seed
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(.2, 0.3, 0.5, 0, 0)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    weights = c(.2, 0.3, 0.5, 0, 0)
    holdings = weights * total_wealth
  }
  wealthtracker
}

head(sim1)

```


Profit/loss for this portfolio
```{r}
par(bg = "gray")
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30,main = "Histogram of Portfolio Returns",xlab = "End of Period Return",ylab = "Frequency",col = "blue")
abline(v = mean(sim1[,n_days]) - initial_wealth,col = "red",lw = 3)
```

Calculating 5% value at risk
```{r}
quantile(sim1[,n_days], 0.05) - initial_wealth
```

Hence for this portfolio the average return is $100531 and value at risk is $3066.526

4) Next we will create an aggressive portfolio which increases our return as well as our risk. Ater trying several combinations, below is the weight we considered for this portfolio:

TLT -  0%
LQD -  0%
SPY -  0%
EEM - 90%
VMQ - 10%

```{r}
initial_wealth = 100000
set.rseed(1)      #Setting seed
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0, 0, 0, .9, .1)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    weights = c(0, 0, 0, .9, .1)
    holdings = weights * total_wealth
  }
  wealthtracker
}

head(sim1)

```

Profit/loss for this portfolio
```{r}
par(bg = "gray")
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=70,main = "Histogram of Portfolio Returns",xlab = "End of Period Return",ylab = "Frequency",col = "blue")
abline(v = mean(sim1[,n_days]) - initial_wealth,col = "red",lw = 3)
```

Calculating 5% value at risk
```{r}
quantile(sim1[,n_days], 0.05) - initial_wealth
```
Hence for this portfolio the average return is $101872.8 and value at risk is $12925.12 

Below is the summary of our results:

Portfolio 1 (Even split):
Mean Return - $100946.6
Value at Risk - $6297.993

Portfolio 2 (Safe):
Mean Return - $100531
Value at Risk - $3066.526 

Portfolio 3 (Risky):
Mean Return $101872.8
Value at Risk - 12925.12 


## NutrientH20 Market Segmentation

First, load the data.
```{r}
df = read.csv("social_marketing.csv")
```

View a sample of the data:
```{r}
head(df)
```
Goal of Data Analysis: Find categories preferred by users so that NutrientH20 can tailor an appropriate marketing message.

Solution: Find the percentage of tweets per category, scale and find the most prominent clusters.


First, change the data frame total tweet numbers to fractions and drop the chatter column. We decided to drop the chatter column because of that column's inability to help shape our marketing message. Because a large number of tweets are characterized as "chatter", this would merely add noise to our clustering model and fail to provide interpretable results.
```{r}
df$X = NULL
df = data.frame(as.matrix(df)/rowSums(as.matrix(df)))
tempDf = df
#drop chatter 
tempDf$chatter= NULL
head(df)
```


Second, let's load the libraries required. This includes registering parallel computing.
```{r include = FALSE}
#set up parallel computing
library(foreach)
library(doParallel)
library(LICORS)
library(clusterCrit)
##register parallel backend
registerDoParallel(cores = 4)
```


Next, run a function _ClusterTweeters_. That function, given a K and a dataframe will return the CH score of all that model. _withoutChatter_ is a matrix with the CH score and SSE(tot) of all the models tested between k = 1 and k = 15. This step included **%dopar%** to tell the foreach loop to compute the results in parallel.
```{r}
clusterTweeters = function(numK,df){
  model = LICORS::kmeanspp(as.matrix(df), k = numK,nstart = 10)
  #find ch value for the model
  c(clusterCrit::intCriteria(as.matrix(df),model$cluster,"Calinski_Harabasz"),model$tot.withinss)
}

withoutChatter = foreach(k = 1:15,.combine = 'rbind')%dopar%
{
  clusterTweeters(k,tempDf)
}
colnames(withoutChatter) = c('CH Score','Fit')
withoutChatter
```





```{r}
par(bg = "gray")
plot(1:nrow(withoutChatter),withoutChatter[,2],col = "red",main = "Fit vs. K",type = "l",xlab = "K",ylab = "SSE (tot)")
```
Using the elbow method, there is no clear shift in slope of the line as K increases. This curve looks closer to an exponential decay than an elbow. The elbow method, therefore, is not a good metric of choosing the optimal number of market segments. A heuristic such as CH score could find a better optimal K.

Let's plot the CH scores of all the models tested. 
```{r}
par(bg = "gray")
plot(1:nrow(withoutChatter),withoutChatter[,1],main = "K selections versus CH Score",xlab = "K",ylab = "CH Score",type = "l",col = "red")
abline(v = (which.max(withoutChatter)+1),col = "blue",lw = 2)
```
CH score indicates two optimal market segments. We believe that this is the best metric for determining the clusters because of the fit vs. simplicity trade off inherent within the heuristic and it's clear answer: two segments.



Next, let's analyze the important components of that each cluster. In other words, for each cluster's centriod, let's look for the centriod with the highest value.

```{r}
optimalModel = LICORS::kmeanspp(tempDf, (which.max(withoutChatter)+1),nstart = 10)
#plot parameters for each model
topN = 5
centriodMat = matrix(NA,2,topN)
par(bg = "gray",mfrow = c(2,1))
for (k in 1:2)
{
  sorted = sort(optimalModel$centers[k,],decreasing = TRUE)[1:topN]
  barplot(sorted,ylim = c(0,.23),col = "blue",main = paste("Figure",k,":"))
  centriodMat[k,] = names(sorted)
}

print(paste("Top",topN,"important centriod indicators per cluster: "))
print(centriodMat)
```

The one of the clusters looks to be composed of people who are very concious of their health. They seem to be focused on Personal Fitness and are interested in the outdoors. This indicates a very physically active group of people and would recommend that the marketing message ought to be related to personal health. This cluster contains this fraction of social media users:

```{r}
length(which(optimalModel$cluster == 1))/length(optimalModel$cluster)
```

The second cluster appears to be grouped for millenials. Politically engaged(current_events and politics), educated (college_uni) and active in photo sharing, this could be a young group of people. We would need to dig more into the profiles of this segment, but a marketing strategy focused on millenials could be appropriate. A visually appealing marketing campaign would be attractive to these social media followers.

This clusters contains this fraction of social media users:

```{r}
length(which(optimalModel$cluster == 2))/length(optimalModel$cluster)
```